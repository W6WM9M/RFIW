{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ebf2ea",
   "metadata": {},
   "source": [
    "## Kinship Classification using Face Transformer Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e8309",
   "metadata": {},
   "source": [
    "**Basic Idea**: Use the features extracted by pretrained Face Transformer to identify whether an image pair are kin or non-kin <br>\n",
    "**Face Transformer Repository**: https://github.com/zhongyy/Face-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import sys\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "import pandas as pd\n",
    "\n",
    "# Download from Face-Transformer Repository\n",
    "from vit_pytorch.vit_face.vit_face import ViT_face\n",
    "from vit_pytorch.vit_face.vits_face import ViTs_face\n",
    "\n",
    "### Paths to change start###\n",
    "ROOT = \"/home/UG/c200140/recognizing-faces-in-the-wild\" # Change this path \n",
    "\n",
    "IMAGE_PATH = ROOT + \"/train\" # Folder of train images\n",
    "\n",
    "# Folder where all features extracted by different models will be stored\n",
    "SENET_FEATURE_PATH = ROOT + \"/train-senet-vgg2-features\"\n",
    "RESNET_FEATURE_PATH = ROOT + \"/train-resnet-vgg2-features\"\n",
    "VIT_FACE_FEATURE_PATH = ROOT + \"/train-vit-face-features\"\n",
    "    \n",
    "TRAIN_FOLDER = ROOT + \"/Excel/Train (New)\" # Folder of training data excel sheets for different validation set\n",
    "TEST_FOLDER = ROOT + \"/Excel/Test (New)\" # Folder of validation data excel sheets for different validation set\n",
    "\n",
    "TEST_FILE_PATH = ROOT + \"/sample_submission.csv\" # Location of sample_submission.csv\n",
    "### Paths to change end###\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f128456",
   "metadata": {},
   "source": [
    "## Step 1: Convert Train Images to Vision Transformer Output Vector and Save Them\n",
    "This eliminates the need to transform the train images everytime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Folders of Each Family and its Members just like the train folder\n",
    "### The folder will now store the extracted features instead of the actual images. \n",
    "def create_folders_for_feature(model_name):\n",
    "    path = None\n",
    "    if model_name == \"senet\":\n",
    "        path = SENET_FEATURE_PATH\n",
    "    elif model_name == \"resnet\":\n",
    "        path = RESNET_FEATURE_PATH\n",
    "    elif model_name == \"vit-face\":\n",
    "        path = VIT_FACE_FEATURE_PATH\n",
    "        \n",
    "    try:\n",
    "        if path != None:\n",
    "            for family in os.listdir(IMAGE_PATH):\n",
    "                os.mkdir(path + f\"/{family}\")\n",
    "                for member in os.listdir(IMAGE_PATH + f\"/{family}\"):\n",
    "                    os.mkdir(path + f\"/{family}/{member}\")\n",
    "    except FileExistsError: \n",
    "        print(\"Folder already exist!\")\n",
    "\n",
    "create_folders_for_feature(\"vit-face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTs_face(\n",
    "            loss_type='CosFace',\n",
    "            GPU_ID=device,\n",
    "            num_class=93431,\n",
    "            image_size=112,\n",
    "            patch_size=8,\n",
    "            ac_patch_size=12,\n",
    "            pad=4,\n",
    "            dim=512,\n",
    "            depth=20,\n",
    "            heads=8,\n",
    "            mlp_dim=2048,\n",
    "            dropout=0.1,\n",
    "            emb_dropout=0.1\n",
    "        )\n",
    "model.load_state_dict(torch.load(f\"{ROOT}/ViT-Face/vits_face.pth\"))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to convert PIL image to a Torch tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)), # ViT-face only accepts size (112, 112)\n",
    "    transforms.PILToTensor(),\n",
    "])\n",
    "\n",
    "# Convert every image in the train folder to a feature vector\n",
    "# prevent gradient calculation for faster extraction\n",
    "with torch.no_grad():\n",
    "    for family in tqdm(os.listdir(IMAGE_PATH), total = len(os.listdir(IMAGE_PATH))):\n",
    "        for member in os.listdir(IMAGE_PATH + f\"/{family}\"):\n",
    "            for img_name in os.listdir(IMAGE_PATH + f\"/{family}/{member}\"):\n",
    "                \n",
    "                # Load the image as Torch tensor\n",
    "                image = transform(Image.open(IMAGE_PATH + f\"/{family}/{member}/{img_name}\").convert('RGB')).float().reshape(1, 3, 112, 112).to(device)\n",
    "                \n",
    "                # Flatten into one-dimensional feature vector\n",
    "                img_feature = model(image).flatten().cpu().numpy()\n",
    "                \n",
    "                # Save into folder\n",
    "                np.save(VIT_FACE_FEATURE_PATH + f\"/{family}/{member}/{Path(img_name).stem}.npy\", img_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84348974",
   "metadata": {},
   "source": [
    "## Step 2: Training Neural Network Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cf551f",
   "metadata": {},
   "source": [
    "#### Functions for Building Training and Validation Dataset from Transformer Output in Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0abc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_row(path1, path2, model_name):\n",
    "    if model_name == \"senet\":\n",
    "        feature1 = np.load(SENET_FEATURE_PATH + f\"/{path1}.npy\").reshape(1, 2048)\n",
    "        feature2 = np.load(SENET_FEATURE_PATH + f\"/{path2}.npy\").reshape(1, 2048)\n",
    "    elif model_name == \"resnet\":\n",
    "        feature1 = np.load(RESNET_FEATURE_PATH + f\"/{path1}.npy\").reshape(1, 2048)\n",
    "        feature2 = np.load(RESNET_FEATURE_PATH + f\"/{path2}.npy\").reshape(1, 2048)\n",
    "    elif model_name == \"vit_face\":\n",
    "        feature1 = np.load(VIT_FACE_FEATURE_PATH + f\"/{path1}.npy\").reshape(1, 512)\n",
    "        feature2 = np.load(VIT_FACE_FEATURE_PATH + f\"/{path2}.npy\").reshape(1, 512)\n",
    "        \n",
    "    feature_pair = np.concatenate([feature1, feature2], axis = 0)\n",
    "    return feature_pair \n",
    "\n",
    "def load_data(val_set, model_name, feature_size):\n",
    "    KIN_CSV_FILE_TRAIN = TRAIN_FOLDER + f\"/train-kin-pairs-{val_set}.csv\"\n",
    "    KIN_CSV_FILE_TEST = TEST_FOLDER + f\"/test-kin-pairs-{val_set}.csv\"\n",
    "    NON_KIN_CSV_FILE_TRAIN = TRAIN_FOLDER + f\"/train-non-kin-pairs-{val_set}.csv\"\n",
    "    NON_KIN_CSV_FILE_TEST = TEST_FOLDER + f\"/test-non-kin-pairs-{val_set}.csv\"\n",
    "\n",
    "    # Loading Kinship and Non-Kinship Dataset\n",
    "    kin_df_train = pd.read_csv(KIN_CSV_FILE_TRAIN)\n",
    "    kin_df_train[\"Label\"] = 1\n",
    "    kin_df_test = pd.read_csv(KIN_CSV_FILE_TEST)\n",
    "    kin_df_test[\"Label\"] = 1\n",
    "    KINSHIP_SIZE_TRAIN = len(kin_df_train) \n",
    "    KINSHIP_SIZE_TEST = len(kin_df_test)\n",
    "\n",
    "    non_kin_df_train = pd.read_csv(NON_KIN_CSV_FILE_TRAIN)\n",
    "    non_kin_df_train[\"Label\"] = 0\n",
    "    non_kin_df_test = pd.read_csv(NON_KIN_CSV_FILE_TEST)\n",
    "    non_kin_df_test[\"Label\"] = 0\n",
    "\n",
    "    train_df = pd.concat([kin_df_train, non_kin_df_train]).reset_index(drop=True)\n",
    "    test_df =  pd.concat([kin_df_test, non_kin_df_test]).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Kinship Dataset Size (Train): {len(kin_df_train)}\")\n",
    "    print(f\"Kinship Dataset Size (Test): {len(kin_df_test)}\")\n",
    "    print(f\"Non-Kinship Dataset Size (Train): {len(non_kin_df_train)}\")\n",
    "    print(f\"Non-Kinship Dataset Size (Test): {len(non_kin_df_test)}\")\n",
    "\n",
    "    print(f\"Train Size: {len(train_df)}\")\n",
    "    print(f\"Test Size: {len(test_df)}\")\n",
    "\n",
    "    \n",
    "    train_dataset = np.empty([KINSHIP_SIZE_TRAIN * 2,2, feature_size], dtype=np.float32)\n",
    "    train_label = np.empty([KINSHIP_SIZE_TRAIN * 2], dtype=np.float32)\n",
    "\n",
    "    for index, row in tqdm(train_df.iterrows(), total = len(train_df), desc=\"Progress\"):\n",
    "        img_pair = build_dataset_row(row[0], row[1], model_name)\n",
    "        train_dataset[index] = img_pair # Doing this is faster than concatenating in a for loop\n",
    "        train_label[index] = row[2]\n",
    "\n",
    "    test_dataset = np.empty([KINSHIP_SIZE_TEST * 2,2,feature_size], dtype=np.float32)\n",
    "    test_label = np.empty([KINSHIP_SIZE_TEST * 2], dtype=np.float32)\n",
    "\n",
    "    for index, row in tqdm(test_df.iterrows(), total = len(test_df), desc=\"Progress\"):\n",
    "        img_pair = build_dataset_row(row[0], row[1], model_name)\n",
    "        test_dataset[index] = img_pair\n",
    "        test_label[index] = row[2]\n",
    "    \n",
    "    train_dataset = torch.tensor(train_dataset, dtype = torch.float32).to(device)\n",
    "    train_label = torch.tensor(train_label, dtype = torch.float32).to(device)\n",
    "    test_dataset = torch.tensor(test_dataset, dtype = torch.float32).to(device)\n",
    "    test_label = torch.tensor(test_label, dtype = torch.float32).to(device)\n",
    "\n",
    "    return train_dataset, train_label, test_dataset, test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68a7cd",
   "metadata": {},
   "source": [
    "#### Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280bead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "class EarlyStopper_Checkpoint():\n",
    "    def __init__(self, patience=1, save_path = None, min_delta=0, metric = \"val_loss\"):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.save_path = save_path\n",
    "        self.metric = metric\n",
    "        if metric == \"val_loss\":\n",
    "            self.min_validation_loss = np.inf\n",
    "        elif metric == \"val_acc\":\n",
    "            self.max_validation_accuracy = 0\n",
    "\n",
    "    def early_stop(self, metric):\n",
    "        if self.metric == \"val_loss\":\n",
    "            if self.check_metric(metric):\n",
    "                self.min_validation_loss = metric\n",
    "                self.counter = 0\n",
    "            elif metric > (self.min_validation_loss + self.min_delta):\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return True\n",
    "            return False\n",
    "        elif self.metric == \"val_acc\":\n",
    "            if self.check_metric(metric):\n",
    "                self.max_validation_accuracy = metric\n",
    "                self.counter = 0\n",
    "            elif metric < (self.max_validation_accuracy + self.min_delta):\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return True\n",
    "            return False\n",
    "            \n",
    "    def check_metric(self, metric):\n",
    "        if self.metric == \"val_loss\":\n",
    "            if metric < self.min_validation_loss:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif self.metric == \"val_acc\":\n",
    "            if metric > self.max_validation_accuracy:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "class NN_Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN_Classifier, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.linear_relu_stack(x)\n",
    "        return output\n",
    "\n",
    "def ema_loss(cur_loss, prev_loss):\n",
    "    loss = 0.9 * prev_loss + 0.1 * cur_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def combine_function(x1, x2, function):\n",
    "    if function == \"diff\":\n",
    "        return (x1 - x2)\n",
    "    elif function == \"diff_square\":\n",
    "        x = (x1 - x2)\n",
    "        return torch.sign(x) * x**2\n",
    "    elif function == \"exp_div\":\n",
    "        return torch.exp(x1)/torch.exp(x2) - torch.exp(x2)/torch.exp(x1)\n",
    "    elif function == \"exp_diff\":\n",
    "        return torch.exp(x1) - torch.exp(x2)\n",
    "    elif function == \"concat\":\n",
    "        return torch.concatenate([x1, x2], axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433308a7",
   "metadata": {},
   "source": [
    "#### Convert all test images into Transformer output to prevent repetition (Do this once only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(TEST_FILE_PATH)\n",
    "\n",
    "# Define a transform to convert PIL \n",
    "# image to a Torch tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "feature_extractor = ViTs_face(\n",
    "        loss_type='CosFace',\n",
    "        GPU_ID=device,\n",
    "        num_class=93431,\n",
    "        image_size=112,\n",
    "        patch_size=8,\n",
    "        ac_patch_size=12,\n",
    "        pad=4,\n",
    "        dim=512,\n",
    "        depth=20,\n",
    "        heads=8,\n",
    "        mlp_dim=2048,\n",
    "        dropout=0.1,\n",
    "        emb_dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "feature_extractor.load_state_dict(torch.load(f\"{ROOT}/ViT-Face/vits_face.pth\"))\n",
    "feature_extractor.eval()\n",
    "\n",
    "img1_features = torch.empty(size = [5310, 512]).to(device)\n",
    "img2_features = torch.empty(size = [5310, 512]).to(device)\n",
    "\n",
    "# prevent gradient calculation for faster extraction\n",
    "with torch.no_grad():\n",
    "    ### Creating Folders of Each Family and its Members just like the test-public-faces folder\n",
    "    for index, row in tqdm(df_test.iterrows(), total = len(df_test), desc=\"Progress\"):\n",
    "        img1, img2 = row[0].split(\"-\")\n",
    "\n",
    "        img1 = transform(Image.open(ROOT + f\"/test/{img1}\").convert('RGB')).float().reshape(1,3, 112, 112).to(device) # Shape: (3, 224, 224) -> RGB (C x H x W)\n",
    "        img2 = transform(Image.open(ROOT + f\"/test/{img2}\").convert('RGB')).float().reshape(1,3, 112, 112).to(device)\n",
    "\n",
    "        # Extract Features\n",
    "        img1_features[index] = feature_extractor(img1).flatten()\n",
    "        img2_features[index] = feature_extractor(img2).flatten()\n",
    "\n",
    "    np.save(ROOT + \"/test-vit-face-features/img1_features.npy\", img1_features.cpu())\n",
    "    np.save(ROOT + \"/test-vit-face-features/img2_features.npy\", img2_features.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33324205",
   "metadata": {},
   "source": [
    "#### Train Neural Network Across Validation Sets and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bab397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for val_set in [\"V00\", \"V01\", \"V02\", \"V03\", \"V04\", \"V05\", \"V06\", \"V07\", \"V08\"]:\n",
    "    # Load Dataset\n",
    "    _X_train, _y_train, _X_test, _y_test = load_data(val_set, \"vit_face\", 512)\n",
    "    \n",
    "    for function in [\"diff\", \"diff_square\", \"exp_diff\", \"exp_div\", \"concat\"]:\n",
    "        X_train = combine_function(_X_train[:, 0, :], _X_train[:, 1, :], function)\n",
    "        X_test = combine_function(_X_test[:, 0, :], _X_test[:, 1, :], function)\n",
    "\n",
    "        X_train = X_train.to(device)\n",
    "        y_train = _y_train.reshape(-1, 1).to(device)\n",
    "        X_test = X_test.to(device)\n",
    "        y_test = _y_test.reshape(-1, 1).to(device)\n",
    "\n",
    "        model_name = \"vit_face\"\n",
    "\n",
    "        print(f\"X_Train Shape: {X_train.shape}\")\n",
    "        print(f\"y_Train Shape: {y_train.shape}\")\n",
    "        print(f\"X_Test Shape: {X_test.shape}\")\n",
    "        print(f\"y_Test Shape: {y_test.shape}\")\n",
    "\n",
    "        print(f\"y_train Class Distribution\")\n",
    "        print(f\"Kinship Pairs: {torch.unique(y_train, return_counts = True)[1][1]}\")\n",
    "        print(f\"Non-Kinship Pairs: {torch.unique(y_train, return_counts = True)[1][0]}\")\n",
    "        print()\n",
    "        print(f\"y_test Class Distribution\")\n",
    "        print(f\"Kinship Pairs: {torch.unique(y_test, return_counts = True)[1][1]}\")\n",
    "        print(f\"Non-Kinship Pairs: {torch.unique(y_test, return_counts = True)[1][0]}\")\n",
    "\n",
    "        batch_size = 256\n",
    "\n",
    "        dataloader_train = DataLoader(TensorDataset(X_train, y_train.float()), \n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True)\n",
    "\n",
    "        dataloader_test =  DataLoader(TensorDataset(X_test, y_test.float()), \n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True)\n",
    "\n",
    "        if function == \"concat\":\n",
    "            model = NN_Classifier(1024, 1024, 1).to(device)\n",
    "        else:\n",
    "            model = NN_Classifier(512, 512, 1).to(device)\n",
    "        checkpoint_name = f\"{model_name}_{function}_{val_set}\"\n",
    "\n",
    "        learning_rate = 1e-4\n",
    "        no_of_epoch = 100\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay=3e-4)\n",
    "        scheduler = LinearLR(optimizer, start_factor=1, end_factor = 0.2, total_iters=50)\n",
    "        start_epoch = 0\n",
    "\n",
    "        early_stopper = EarlyStopper_Checkpoint(patience = 5, save_path =  ROOT + f\"/Model Checkpoints/ViT Face/Without V09/{checkpoint_name}_best.pt\", metric = \"val_acc\")\n",
    "\n",
    "        bce_loss_func = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "        for epoch in range(start_epoch, start_epoch + no_of_epoch):\n",
    "            train_loss = 0\n",
    "            correct = 0\n",
    "\n",
    "            # Training \n",
    "            model.train()\n",
    "            for train_data, train_label in dataloader_train:\n",
    "                optimizer.zero_grad()\n",
    "                output= model(train_data)\n",
    "                loss = bce_loss_func(output, train_label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss = ema_loss(loss, train_loss)\n",
    "\n",
    "                y_pred = nn.Sigmoid()(output.detach())\n",
    "\n",
    "                correct += ((y_pred>0.5).float() == train_label).float().sum()\n",
    "\n",
    "            train_accuracy = correct / len(dataloader_train.dataset) * 100\n",
    "            checkpoint = {\"epoch\": epoch,\n",
    "                          \"model_state_dict\": model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'scheduler_state_dict': scheduler.state_dict()}\n",
    "            torch.save(checkpoint, f\"./Model Checkpoints/ViT Face/Without V09/{checkpoint_name}.pt\")\n",
    "\n",
    "            print(f\"Saving checkpoint to {checkpoint_name}.pt\")\n",
    "            correct = 0\n",
    "            test_loss = 0\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for test_data, test_label in dataloader_test:\n",
    "                    output= model(test_data)\n",
    "                    loss = bce_loss_func(output, test_label)\n",
    "                    y_pred = nn.Sigmoid()(output)\n",
    "                    test_loss = ema_loss(loss, test_loss)\n",
    "                    correct += ((y_pred>0.5).float() == test_label).float().sum()\n",
    "\n",
    "                test_accuracy = correct / len(dataloader_test.dataset) * 100\n",
    "                print(f\"Epoch {epoch}: Train Loss: {train_loss: .5f}, Training Accuracy: {train_accuracy:.2f}%, Test Loss: {test_loss: .5f}, Testing Accuracy = {test_accuracy:.2f}%\")\n",
    "\n",
    "            scheduler.step()\n",
    "            if early_stopper.check_metric(test_accuracy):\n",
    "                checkpoint = {\"epoch\": epoch,\n",
    "                              \"test_loss\": test_loss,\n",
    "                              \"test_acc\": test_accuracy,\n",
    "                              \"model_state_dict\": model.state_dict(),\n",
    "                              'optimizer_state_dict': optimizer.state_dict(),\n",
    "                              'scheduler_state_dict': scheduler.state_dict()}\n",
    "                torch.save(checkpoint, f\"./Model Checkpoints/ViT Face/Without V09/{checkpoint_name}_best.pt\")\n",
    "                print(f\"Saving checkpoint to {checkpoint_name}_best.pt\")\n",
    "\n",
    "\n",
    "            if early_stopper.early_stop(test_accuracy):\n",
    "                print(\"Stopped early due to no improvement in validation loss\")\n",
    "                break\n",
    "\n",
    "        print(checkpoint_name)\n",
    "        if function == \"concat\":\n",
    "            model = NN_Classifier(1024, 1024, 1).to(device)\n",
    "        else:\n",
    "            model = NN_Classifier(512, 512, 1).to(device)\n",
    "        model.load_state_dict(torch.load(ROOT + f\"/Model Checkpoints/ViT Face/Without V09/{checkpoint_name}_best.pt\")[\"model_state_dict\"])\n",
    "        model.eval()\n",
    "\n",
    "        # Loading the test dataset\n",
    "        df_test = pd.read_csv(TEST_FILE_PATH)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img1_features = torch.tensor(np.load(ROOT + \"/test-vit-face-features/img1_features.npy\")).to(device)\n",
    "            img2_features = torch.tensor(np.load(ROOT + \"/test-vit-face-features/img2_features.npy\")).to(device)\n",
    "            X = combine_function(img1_features, img2_features, function)\n",
    "            output = model(X)\n",
    "            y_pred = nn.Sigmoid()(output)\n",
    "            df_test['is_related'] = y_pred.detach().cpu()\n",
    "        \n",
    "        # Print Distribution of Output\n",
    "        print(f\"0 - 0.1 : {(df_test['is_related']<=0.1).sum()}\")\n",
    "        print(f\"0.1 - 0.2 : {(df_test['is_related']<=0.2).sum()}\")\n",
    "        print(f\"0.2 - 0.3 : {(df_test['is_related']<=0.3).sum()}\")\n",
    "\n",
    "        print(f\"0.3 - 0.4 : {(df_test['is_related']<=0.4).sum()}\")\n",
    "        print(f\"0.4 - 0.5 : {(df_test['is_related']<0.5).sum()}\")\n",
    "\n",
    "        print(f\"0.5 - 0.6 : {(df_test['is_related']>=0.5).sum()}\")\n",
    "        print(f\"0.6 - 0.7 : {(df_test['is_related']>=0.6).sum()}\")\n",
    "        print(f\"0.7 - 0.8 : {(df_test['is_related']>=0.7).sum()}\")\n",
    "        print(f\"0.8 - 0.9 : {(df_test['is_related']>=0.8).sum()}\")\n",
    "        print(f\"0.9 - 1 : {(df_test['is_related']>=0.9).sum()}\")\n",
    "        \n",
    "        df_test.to_csv(ROOT + f\"/Test Results/ViT-Face/Without V09/{checkpoint_name}.csv\", index = False)\n",
    "        model = None\n",
    "        optimizer = None\n",
    "        scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd27ec",
   "metadata": {},
   "source": [
    "### Step 3: Creating an Ensemble via Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9eae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for function in [\"diff\", \"diff_square\", \"exp_diff\", \"exp_div\", \"concat\"]:\n",
    "    df_test = pd.read_csv(TEST_FILE_PATH)\n",
    "    for val_set in [\"V00\", \"V01\", \"V02\", \"V03\", \"V04\", \"V05\", \"V06\", \"V07\", \"V08\"]:\n",
    "        df_test['is_related'] += pd.read_csv(ROOT + f\"/Test Results/ViT-Face/Without V09/vit_face_{function}_{val_set}.csv\")['is_related']/9\n",
    "    \n",
    "    df_test.to_csv(ROOT + f\"/Test Results/ViT-Face/Without V09/vit_face_{function}_avg.csv\", index = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(TEST_FILE_PATH)\n",
    "for function in [\"diff\", \"diff_square\", \"exp_diff\", \"exp_div\", \"concat\"]:\n",
    "    df_test['is_related'] += pd.read_csv(ROOT + f\"/Test Results/ViT-Face/Without V09/vit_face_{function}_avg.csv\")['is_related']/5\n",
    "df_test.to_csv(ROOT + f\"/Test Results/ViT-Face/Without V09/vit_face_avg.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80711503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
